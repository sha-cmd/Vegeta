{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 2 Participez à un concours sur la Smart City\n",
    "\n",
    "## Jupyter Notebook\n",
    "\n",
    "Nous utilisons ce programme, qui est un outil informatique de logiciel libre, qui est basé sur IPython. Il vous permettra de lire les commentaires écrit au format markdown, et d'interpréter les cellules de code, ici en Python. Mais cette technologie marche avec d'autres langages, Scala par exemple.\n",
    "\n",
    "## Pandas\n",
    "\n",
    "Nous utiliserons le langage Python, conformément à la consigne, et nous importerons la bibliothèque de manipulation de données Pandas. Ensuite nous importerons Numpy pour travailler avec des matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "import seaborn as sns\n",
    "\n",
    "import colorcet\n",
    "import itertools\n",
    "import math\n",
    "import openpyxl\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "from collections import OrderedDict, deque\n",
    "from IPython.display import HTML as html_print\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Présentation générale du jeu de données\n",
    "\n",
    "## Data requirements\n",
    "\n",
    "La ville de Paris veut optimiser ses tournées, il s'agit d'un problème d'algorithmie courant : ***Problème du voyageur de commerce***. Aujourd'hui ce calcul peut s'effectuer avec plusieurs millions de ville sans que cela ne prenne trop de temps de calcul. La ville de Paris met à notre disposition les positions de chaque emplacement et leur nom, de sorte que si nous considérons que le réseau de Paris est assez dense pour que nous puissions négliger les sinuosités de la route, nous pouvons calculer la distance à vol d'oiseau, entre les emplacements. Et en prenant l'emplacement le plus proche de l'emplacement ou nous sommes, nous pouvons espérer trouver ainsi le plus court chemin pour parvenir à notre objectif de tournée.\n",
    "\n",
    "## Data collection\n",
    "\n",
    "Nous récupérons les données depuis une seule source, un fichier \"comma separator value\" ou csv, [à cette adresse](https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+2+Participez+%C3%A0+un+concours+sur+la+Smart+City/p2-arbres-fr.csv). Nous plaçons ensuite le fichier dans le répertoire de travail pour l'importer dans la partie suivante. Ce jeu de données comporte $17$ colonnes. $200137$ lignes et $646164$ valeurs manquantes. La colonne numéro est inutilisable, et celle des id ne démarre pas à *0*. Plus d'information seront détaillées dans les paragraphes suivants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Nous obtenons les données dans le bon format, mais avec quelques erreurs, nous verrons cela plus tard, pour l'instant nous importons nos données en indiquant le séparateur, et la colonne servant à indexer la table de nos données. Nous remplaçons la colonne d'indexation par un index à l'itération parfaite. Nous éliminons les colonnes id et numero qui ne sont pas intéressante pour nos observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', sep=';')\n",
    "data.drop('id', axis=1, inplace=True)\n",
    "data.drop('numero', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Nous devons contrôler les doublons, les valeurs nulles, les données incomplètes, les erreurs et les données manquantes. Nous réglons les types, pour obtenir des valeurs à mettre en base de données, au besoin.\n",
    "Voilà comment se présentent les données sorties du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les valeurs nulles, les types.\n",
    "\n",
    "Nous remplaçons les valeurs nulles de la colonne \"remarquable\" par la valeur 0 (qui code pour False), et nous pouvons remplacer le type par un booléen après avoir échanger les valeurs flottantes par des booleénnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['remarquable'].fillna(value=0, inplace=True)\n",
    "data['remarquable'] = data['remarquable'].map({0.:False, 1.:True})\n",
    "data['remarquable'] = data['remarquable'].convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les colonnes \"arrondissement\"…, qui n'ont pas de valeurs nulles, nous forçons la conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes = ['arrondissement', 'type_emplacement', 'lieu', 'id_emplacement']\n",
    "for col in colonnes:\n",
    "    data[col] = data[col].convert_dtypes(convert_string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les colonnes \"stade_developpement\"…, comme ce sont des strings, nous remplaçons les valeurs nulles par deux quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes = ['stade_developpement', 'espece', 'variete', 'genre', 'libelle_francais',\\\n",
    "            'complement_addresse', 'domanialite']\n",
    "for col in colonnes:\n",
    "    data[col].fillna(value=\"\", inplace=True)\n",
    "    data[col] = data[col].convert_dtypes(convert_string=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construire une table d'information personnalisée\n",
    "\n",
    "Il est possible de se passer de la méthode info pour construire un algorithme plus puissant afin d'observer le plus d'information sur les tables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informations(data):\n",
    "    # Header\n",
    "    print('{:^3}{:<1}{:^20}{:<1}{:^9}{:>1}{:^6}{:<1}{:^15}{:<1}{:^7}{:<1}{:^7}{:<1}'.format('#' , '|' , 'Column', '|', 'Dtype', '|', 'Unique', '|', 'Count Non-Null','|','Mean','|','Std','|'))\n",
    "    print('{:^3}{:<1}{:^20}{:<1}{:^9}{:>1}{:^6}{:<1}{:^15}{:<1}{:^7}{:<1}{:^7}{:<1}'.format('---' , '|' , '------', '|', '-----', '|', '------', '|', '--------------','|','----','|','---','|'))\n",
    "    # dtypes information\n",
    "    dtypes_uniques = set() # Collection of unique elements\n",
    "    dtypes_listes = [] # list of complete data columns\n",
    "    memory_usage = 0 # in MB\n",
    "    for it, col in enumerate(data.columns): # Feed the set and the list\n",
    "        dtypes_listes.append(str(data[col].dtype))\n",
    "        if str(data[col].dtype) not in dtypes_uniques:\n",
    "            dtypes_uniques.add(str(data[col].dtype))\n",
    "        # Table body\n",
    "        print('{:^3}{:<1}{:<20}{:<1}{:^9}{:>1}{:<6}{:<1}{:<15}{:<1}{:<7.5}{:<1}{:<7.5}{:<1}'.format(\\\n",
    "                                                str(it), '|' , col, '|',\\\n",
    "                                                str(data[col].dtype), '|', str(len(data[col].unique())),\\\n",
    "                                                '|',str(data[col].count()) + ' Non-null','|',\\\n",
    "                                                str(data[col].mean())\\\n",
    "                                                    if (data[col].dtype in ['int64','float64']) else '','|',\\\n",
    "                                                str(data[col].std())\\\n",
    "                                                    if (data[col].dtype in ['int64','float64']) else '','|'))\n",
    "        \n",
    "        # Collect informatives on disk usage by observation onto the data column\n",
    "        memory_usage += int(data[col].memory_usage(index=True, deep=False))\n",
    "    # Blend of set and list to print the information line as usual\n",
    "    dtypes_string = ''\n",
    "    for x in dtypes_uniques:\n",
    "        dtypes_string += '{}({}), '.format(x, dtypes_listes.count(x))\n",
    "    print('\\ndtypes: {}'.format(dtypes_string))\n",
    "    # Digit format to write mem usage in comprehensive format\n",
    "    print('\\nmemory usage: {:.4} MB\\n'.format(memory_usage / (1024*1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informations(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Démarche méthodologique d'analyse de données\n",
    "\n",
    "## Analyse univarée\n",
    "\n",
    "Notre analyse va nous servir à détecter les erreurs dans la base de données, qui sont autant anomalies (\"outliers\") ou de données aberrantes qui peuvent nous conduire à de fausses conclusions sur les éléments de notre analyse. Nous analyserons les colonnes circonférence en centimètre, et hauteur en mètre. Les ordres de grandeurs sont indiqués dans les lignes min/max du tableau ci-dessous. À noter que nous commençons d'abord par décrire les données sans filtres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['circonference_cm','hauteur_m']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervalle interquartile\n",
    "\n",
    "$Q_i = Q_3 - Q_1$ nous donne l'intervalle interquartile. L'intervalle minimum de confiance est $Q_i \\times 1.5$, c'est la limite intérieur. De plus l'intervalle maximum de confiance est $Q_i \\times 3$. Nous réglons nos bornes selon cette formule : pour la circonférence le maximum est $255$, alors que pour la hauteur le maximum est $21$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessus, la ligne max nous permet de raisonner sur la présence d'aberrations et dans les prochains paragraphes, en ajustant les valeurs d'exclusions en multipliant par 3,5 , nous allons faire une analyse univariée pour la circonférence et la hauteur des arbres de la ville de Paris. Nous avons pris la circonférence entre $]0, 255] centimètres$, la hauteur entre $]0, 21] mètres$, puis les remarquables avec une valeur Vrai (dans un cas spécialement adapté ou la corrélation est évaluée entre la hauteur et la circonférence).\n",
    "\n",
    "### La circonférence\n",
    "\n",
    "D'après le graphique et les résultats qui suivent, la circonférence de la population est une distribution dont le maximum est de $400 cm$ pour une moyenne de 91 cm et un écart type de $57,59 cm$ de part et d'autre de la moyenne. L'allure et la répartition de la distribution de la population est une asymétrie vers la droite, et le plus fréquemment, la circonférence est de 20 cm. L'échantillon considéré est de $174026$ arbres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.loc[(data[\"circonference_cm\"] <= 255) & (data[\"circonference_cm\"] > 0)]\n",
    "sns.displot(data=a, x=\"circonference_cm\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"circonference_cm\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"circonference_cm\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.skew(a[\"circonference_cm\"], axis=0, bias=True, nan_policy='propagate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.kurtosis(a[\"circonference_cm\"], axis=0, bias=True, nan_policy='propagate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La hauteur\n",
    "\n",
    "La distribution est irrégulière, mais contient $160122$ arbres après filtrage des données aberrantes. La hauteur maximale est $29 mètres$ et la moyenne est de $10,32 mètres$, tout comme le mode et la médiane. Par contre l'asymétrie est positive, donc vers la droite mais sa valeur est inférieur à $1$, comme l'est aussi son kurtosis. Visuellement, il serait difficile de dire qu'ici s'applique le théorème de limite centrale car nous n'avons pas a première vue une Gaussienne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = data.loc[(data[\"hauteur_m\"] < 21) & (data[\"hauteur_m\"] > 0)]\n",
    "sns.displot(data=b, x=\"hauteur_m\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[\"hauteur_m\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[\"hauteur_m\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.skew(b[\"hauteur_m\"], axis=0, bias=True, nan_policy='propagate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.kurtosis(b[\"hauteur_m\"], axis=0, bias=True, nan_policy='propagate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression linéaire\n",
    "\n",
    "En considérant l'expression de la circonférence d'après la hauteur, nous pouvons constater une corrélation positive entre le données quantitative, autant pour les arbres remarquables, que les premiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot( x=\"hauteur_m\", y=\"circonference_cm\", data=data.loc[((data[\"hauteur_m\"] < 21) & (data[\"hauteur_m\"] > 0)) \\\n",
    "                          & ((data[\"circonference_cm\"] < 255) & (data[\"circonference_cm\"] > 0)) & (data[\"remarquable\"] == True)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous ajoutons les variables indépendantes circonférence, hauteur pour les arbres remarquables, nous pourrions sans doute appliquer le théorème de limite centrale sur nos données ainsi filtrés tant les courbes ont une forme dont la régularité s'approche plus d'une gaussienne. Peut-être nous restera-t-il à croiser les données avec d'autres pour affiner la courbe en forme de cloche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data.loc[((data[\"hauteur_m\"] < 30) & (data[\"hauteur_m\"] > 0)) \\\n",
    "                          & ((data[\"circonference_cm\"] < 400) & (data[\"circonference_cm\"] > 0)) & data[\"remarquable\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[[\"hauteur_m\",\"circonference_cm\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la circonférence des arbres remarquables, le plus fréquemment, elle vaut $200 cm$ et $15 m$ pour la hauteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=c, x=\"circonference_cm\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"circonference_cm\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.skew(c[\"circonference_cm\"], axis=0, bias=True, nan_policy='propagate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.kurtosis(c[\"circonference_cm\"], axis=0, bias=True, nan_policy='propagate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=c, x=\"hauteur_m\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La description des valeurs du graphique ci-dessus est donnée ci-dessous. Elle nous permet de comparer **les ordres de grandeur** entre les colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"hauteur_m\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.skew(c[\"hauteur_m\"], axis=0, bias=True, nan_policy='propagate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.kurtosis(c[\"hauteur_m\"], axis=0, bias=True, nan_policy='propagate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Analyse Quantitative\n",
    "\n",
    "Nous utilisons un filtre sur la base pour éliminer les arbres dont nous ne sommes pas sur de la hauteur ou de la circonférence, puis avec une requête SQL, nous déterminons le nombre d'individu, dans chaque élément quantitatif à l'intérieur de chaque quartier, excepté pour la domanialité qui est une données de catégorie. Nous ajoutons en plus des moyennes ainsi que des pourcentages du nombre d'arbres. Nous commençons notre analyse quantitative sur $159568$ arbres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data.loc[((data[\"hauteur_m\"] < 30) & (data[\"hauteur_m\"] > 0)) \\\n",
    "                          & ((data[\"circonference_cm\"] < 400) & (data[\"circonference_cm\"] > 0))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus nous remarquons qu'une ligne ne comporte pas de domanialité, mais qu'il semble d'après son lieu, que ce soit un jardin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.loc[data_filtered['domanialite'] == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data_filtered['domanialite'] == ''\n",
    "data_filtered.loc[mask,'domanialite'] = 'Jardin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.loc[data_filtered['domanialite'] == \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même pour la variété, pour 63000 arbres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data_filtered['variete'] == ''\n",
    "data_filtered.loc[mask,'variete'] = 'n. sp.'\n",
    "mask = data_filtered['espece'] == ''\n",
    "data_filtered.loc[mask,'espece'] = 'n. sp.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus pour la légende des stades de développement, nous rajoutons la P, pour particulier, car il représente une masse d'arbre qui sorte des statistiques pour la moitié d'entre eux. Cette catégorie passe de 60000 arbres à 30000 arbres après filtrage des hauteurs et des circonférences. Cette catégorie semble regroupé des arbres de catégories JA et J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.stade_developpement = data.stade_developpement.map({'':'P', 'A':'A','JA':'JA','M':'M','J':'J'})\n",
    "data_filtered.stade_developpement = data_filtered.stade_developpement.map({'':'P', 'A':'A','JA':'JA','M':'M','J':'J'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.stade_developpement.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme la colonne de stade de développement est une variable qualitative ordonnée, nous créons une colonne de sa valeur en chiffre pour un usage quantitatif de la variable car l'âge est quantitatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered['maturite'] = data_filtered.stade_developpement.map({'P':2, 'A':4,'JA':3,'M':5,'J':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_before_filtering = data['type_emplacement'].loc[data.stade_developpement == ''].count()\n",
    "count_after_filtering = data_filtered['type_emplacement'].loc[data_filtered.stade_developpement == 'P'].count()\n",
    "print('Avant : ', count_before_filtering, 'arbres, Après filtrage : ', count_after_filtering, 'arbres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"\"\"SELECT  arrondissement,\n",
    "                ROUND(Count(type_emplacement)* 100. / (Select Count(type_emplacement) From data), 2) as \"pourcent\",\n",
    "                count(DISTINCT(domanialite)) as domani,\n",
    "                count(DISTINCT(lieu)) as lieu,\n",
    "                count(type_emplacement) as arbres,\n",
    "                count(DISTINCT(id_emplacement)) as id_empl,\n",
    "                count(DISTINCT(libelle_francais)) as libel_fr,\n",
    "                count(DISTINCT(genre)) as genre,\n",
    "                count(DISTINCT(espece)) as espece,\n",
    "                count(DISTINCT(variete)) as variete,\n",
    "                ROUND(AVG(circonference_cm),2) as circon,\n",
    "                ROUND(AVG(hauteur_m),2) as haut,\n",
    "                SUM(remarquable) as remarq\n",
    "                FROM data_filtered GROUP BY data_filtered.arrondissement\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_arr = ps.sqldf(q1, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "df = quant_arr[['haut','circon', 'arbres', 'arrondissement']]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(6.5, 6.5))\n",
    "sns.despine(f, left=True, bottom=True)\n",
    "#clarity_ranking = ['M','A','JA','P','J']\n",
    "sns.scatterplot(x=\"circon\", y=\"haut\",\n",
    "                hue=\"arrondissement\", #hue_order=clarity_ranking,\n",
    "                size=\"arbres\",\n",
    "                data=df, ci=\"sd\",ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous faisons la même chose pour chaque domanialité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = \"\"\"SELECT  domanialite,\n",
    "                ROUND(Count(type_emplacement)* 100. / (Select Count(type_emplacement) From data),2) as \"pourcent\",\n",
    "                count(DISTINCT(arrondissement)) as arrond,\n",
    "                count(DISTINCT(stade_developpement)) as devel,\n",
    "                count(DISTINCT(lieu)) as lieu,\n",
    "                count(type_emplacement) as arbres,\n",
    "                count(DISTINCT(id_emplacement)) as id_empl,\n",
    "                count(DISTINCT(libelle_francais)) as libel_fr,\n",
    "                count(DISTINCT(genre)) as genre,\n",
    "                count(DISTINCT(espece)) as espece,\n",
    "                count(DISTINCT(variete)) as variete,\n",
    "                ROUND(AVG(circonference_cm),2) as circon,\n",
    "                ROUND(AVG(hauteur_m),2) as haut,\n",
    "                SUM(remarquable) as remarq\n",
    "                FROM data_filtered GROUP BY data_filtered.domanialite\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_dom = ps.sqldf(q2, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_dom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que le stade de développement peut indiquer une relation avec les produits à utiliser, donc nous utilisons le stade de développement comme pivot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = \"\"\"SELECT  stade_developpement,\n",
    "                ROUND(Count(type_emplacement)* 100. / (Select Count(type_emplacement) From data),2) as \"pourcent\",\n",
    "                count(DISTINCT(arrondissement)) as arrond,\n",
    "                count(DISTINCT(domanialite)) as domani,\n",
    "                count(DISTINCT(lieu)) as lieu,\n",
    "                count(type_emplacement) as arbres,\n",
    "                count(DISTINCT(id_emplacement)) as id_empl,\n",
    "                count(DISTINCT(libelle_francais)) as libel_fr,\n",
    "                count(DISTINCT(genre)) as genre,\n",
    "                count(DISTINCT(espece)) as espece,\n",
    "                count(DISTINCT(variete)) as variete,\n",
    "                ROUND(AVG(circonference_cm), 2) as circon,\n",
    "                ROUND(AVG(hauteur_m), 2) as haut,\n",
    "                SUM(remarquable) as remarq\n",
    "                FROM data_filtered GROUP BY data_filtered.stade_developpement\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_dev = ps.sqldf(q3, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous exprimons les quantités par stade de développement, et par domanialité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4 = \"\"\"SELECT  stade_developpement as devel,\n",
    "                domanialite as domani,\n",
    "                ROUND(Count(type_emplacement)* 100. / (Select Count(type_emplacement) From data), 2) as \"pourcent\",\n",
    "                count(DISTINCT(arrondissement)) as arrond,\n",
    "                count(DISTINCT(lieu)) as lieu,\n",
    "                count(type_emplacement) as arbres,\n",
    "                count(DISTINCT(id_emplacement)) as id_empl,\n",
    "                count(DISTINCT(libelle_francais)) as libel_fr,\n",
    "                count(DISTINCT(genre)) as genre,\n",
    "                count(DISTINCT(espece)) as espece,\n",
    "                count(DISTINCT(variete)) as variete,\n",
    "                ROUND(AVG(circonference_cm),2) as circon,\n",
    "                ROUND(AVG(hauteur_m),2) as haut,\n",
    "                SUM(remarquable) as remarq\n",
    "                FROM data_filtered GROUP BY data_filtered.stade_developpement, domanialite\n",
    "                ORDER BY devel, domani, haut\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_dev_dom = ps.sqldf(q4, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_dev_dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "df = quant_dev_dom[['haut','circon', 'arbres', 'devel']]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(6.5, 6.5))\n",
    "sns.despine(f, left=True, bottom=True)\n",
    "clarity_ranking = ['M','A','JA','P','J']\n",
    "sns.scatterplot(x=\"haut\", y=\"circon\",\n",
    "                hue=\"devel\", hue_order=clarity_ranking,\n",
    "                size=\"arbres\",\n",
    "                data=df, ci=\"sd\",ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5 = \"\"\"SELECT  genre,\n",
    "                ROUND(Count(type_emplacement)* 100. / (Select Count(type_emplacement) From data),2) as \"pourcent\",\n",
    "                count(DISTINCT(arrondissement)) as arrond,\n",
    "                count(DISTINCT(domanialite)) as domani,\n",
    "                count(DISTINCT(lieu)) as lieu,\n",
    "                count(type_emplacement) as arbres,\n",
    "                count(DISTINCT(id_emplacement)) as id_empl,\n",
    "                count(DISTINCT(libelle_francais)) as libel_fr,\n",
    "                count(DISTINCT(variete)) as variete,\n",
    "                count(DISTINCT(espece)) as espece,\n",
    "                ROUND(AVG(maturite), 2) as cat_age,\n",
    "                ROUND(AVG(circonference_cm), 2) as circon,\n",
    "                ROUND(AVG(hauteur_m), 2) as haut,\n",
    "                SUM(remarquable) as remarq\n",
    "                FROM data_filtered GROUP BY data_filtered.genre\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_gen = ps.sqldf(q5, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = quant_gen[['haut','circon','cat_age','arbres']].values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={0:'haut',1:'circon',2:'cat_age',3:'arbres'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_gen = df.merge(quant_gen['genre'], on=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_gen.drop('key_0',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Load the dataset\n",
    "crashes = quant_gen[['genre','arbres','cat_age','haut', 'circon']]#sns.load_dataset(\"car_crashes\")\n",
    "\n",
    "# Make the PairGrid\n",
    "g = sns.PairGrid(crashes.sort_values(\"circon\", ascending=True),\n",
    "                 x_vars=crashes.columns[1:], y_vars=[\"genre\"],\n",
    "                 height=20, aspect=.25)\n",
    "\n",
    "# Draw a dot plot using the stripplot function\n",
    "g.map(sns.stripplot, size=10, orient=\"h\", jitter=False,\n",
    "      palette=\"flare_r\", linewidth=1, edgecolor=\"w\")\n",
    "\n",
    "# Use the same x axis limits on all columns and add better labels\n",
    "g.set(xlim=(0, 1), xlabel='Norme', ylabel=\"\")\n",
    "\n",
    "# Use semantically meaningful titles for the columns\n",
    "titles = ['nb arbres','age','haut', 'circonférence']\n",
    "\n",
    "for ax, title in zip(g.axes.flat, titles):\n",
    "\n",
    "    # Set a different title for each axes\n",
    "    ax.set(title=title)\n",
    "\n",
    "    # Make the grid horizontal instead of vertical\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "g.savefig(\"genre_circonf.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Load the dataset\n",
    "crashes = quant_gen[['genre','arbres','cat_age','haut', 'circon']]#sns.load_dataset(\"car_crashes\")\n",
    "\n",
    "# Make the PairGrid\n",
    "g = sns.PairGrid(crashes.sort_values(\"haut\", ascending=True),\n",
    "                 x_vars=crashes.columns[1:], y_vars=[\"genre\"],\n",
    "                 height=20, aspect=.25)\n",
    "\n",
    "# Draw a dot plot using the stripplot function\n",
    "g.map(sns.stripplot, size=10, orient=\"h\", jitter=False,\n",
    "      palette=\"flare_r\", linewidth=1, edgecolor=\"w\")\n",
    "\n",
    "# Use the same x axis limits on all columns and add better labels\n",
    "g.set(xlim=(0, 1), xlabel='Norme', ylabel=\"\")\n",
    "\n",
    "# Use semantically meaningful titles for the columns\n",
    "titles = ['nb arbres','age','hauteur', 'circonférence']\n",
    "\n",
    "for ax, title in zip(g.axes.flat, titles):\n",
    "\n",
    "    # Set a different title for each axes\n",
    "    ax.set(title=title)\n",
    "\n",
    "    # Make the grid horizontal instead of vertical\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "g.savefig(\"genre_hauteur.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Load the dataset\n",
    "crashes = quant_gen[['genre','arbres','cat_age','haut', 'circon']]#sns.load_dataset(\"car_crashes\")\n",
    "\n",
    "# Make the PairGrid\n",
    "g = sns.PairGrid(crashes.sort_values(\"cat_age\", ascending=True),\n",
    "                 x_vars=crashes.columns[1:], y_vars=[\"genre\"],\n",
    "                 height=20, aspect=.25)\n",
    "\n",
    "# Draw a dot plot using the stripplot function\n",
    "g.map(sns.stripplot, size=10, orient=\"h\", jitter=False,\n",
    "      palette=\"flare_r\", linewidth=1, edgecolor=\"w\")\n",
    "\n",
    "# Use the same x axis limits on all columns and add better labels\n",
    "g.set(xlim=(0, 1), xlabel='Norme', ylabel=\"\")\n",
    "\n",
    "# Use semantically meaningful titles for the columns\n",
    "titles = ['nb arbres','age','haut', 'circonférence']\n",
    "\n",
    "for ax, title in zip(g.axes.flat, titles):\n",
    "\n",
    "    # Set a different title for each axes\n",
    "    ax.set(title=title)\n",
    "\n",
    "    # Make the grid horizontal instead of vertical\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "g.savefig(\"genre_age.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Load the dataset\n",
    "crashes = quant_gen[['genre','arbres','cat_age','haut', 'circon']]#sns.load_dataset(\"car_crashes\")\n",
    "\n",
    "# Make the PairGrid\n",
    "g = sns.PairGrid(crashes.sort_values(\"arbres\", ascending=True),\n",
    "                 x_vars=crashes.columns[1:], y_vars=[\"genre\"],\n",
    "                 height=20, aspect=.25)\n",
    "\n",
    "# Draw a dot plot using the stripplot function\n",
    "g.map(sns.stripplot, size=10, orient=\"h\", jitter=False,\n",
    "      palette=\"flare_r\", linewidth=1, edgecolor=\"w\")\n",
    "\n",
    "# Use the same x axis limits on all columns and add better labels\n",
    "g.set(xlim=(0, 1), xlabel='Norme', ylabel=\"\")\n",
    "\n",
    "# Use semantically meaningful titles for the columns\n",
    "titles = ['nb arbres','age','haut', 'circonférence']\n",
    "\n",
    "for ax, title in zip(g.axes.flat, titles):\n",
    "\n",
    "    # Set a different title for each axes\n",
    "    ax.set(title=title)\n",
    "\n",
    "    # Make the grid horizontal instead of vertical\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "g.savefig(\"genre_nb_arbres.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Synthèse de mon analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "df = quant_dev_dom[['haut','circon', 'arbres', 'devel']]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(6.5, 6.5))\n",
    "sns.despine(f, left=True, bottom=True)\n",
    "clarity_ranking = ['M','A','JA','P','J']\n",
    "sns.scatterplot(x=\"haut\", y=\"circon\",\n",
    "                hue=\"devel\", hue_order=clarity_ranking,\n",
    "                size=\"arbres\",\n",
    "                data=df, ci=\"sd\",ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "df = quant_arr[['haut','circon', 'arbres', 'arrondissement']]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(6.5, 6.5))\n",
    "sns.despine(f, left=True, bottom=True)\n",
    "#clarity_ranking = ['M','A','JA','P','J']\n",
    "sns.scatterplot(x=\"circon\", y=\"haut\",\n",
    "                hue=\"arrondissement\", #hue_order=clarity_ranking,\n",
    "                size=\"arbres\",\n",
    "                data=df, ci=\"sd\",ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "df = quant_dom[['haut','circon', 'arbres', 'domanialite']]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(6.5, 6.5))\n",
    "sns.despine(f, left=True, bottom=True)\n",
    "#clarity_ranking = ['M','A','JA','P','J']\n",
    "sns.scatterplot(x=\"circon\", y=\"haut\",\n",
    "                hue=\"domanialite\", #hue_order=clarity_ranking,\n",
    "                size=\"arbres\",\n",
    "                data=df, ci=\"sd\",ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carte des positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cstr(s, color='green'):\n",
    "    return \"<text style=color:{}>{}</text>\".format(color,s)\n",
    "html_print(\"Some \" + cstr('green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couleurs_list = ['#4ffb28',\n",
    "         '#5fa390',\n",
    "         '#ff1515',\n",
    "         '#694231',#'#5b3d2a',\n",
    "         '#000000'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couleurs_dict = dict()\n",
    "for it, val in enumerate(data['stade_developpement'].unique()):\n",
    "    couleurs_dict.update({val: couleurs_list[it]})\n",
    "couleurs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['couleurs'] = data['stade_developpement'].map(couleurs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_print(cstr('J: #ff1515', '#ff1515') + \",\\n\"\n",
    " +cstr(\"P: #4ffb28\", \"#4ffb28\")+',\\n'\n",
    " +cstr('JA: #000000', '#000000')+',\\n'\n",
    " +cstr('A: #5fa390', '#5fa390')+',\\n'\n",
    " +cstr('M: #694231', '#694231')+'}\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = ds.Canvas(plot_width=1000, plot_height=1000)\n",
    "agg = cvs.points(data, 'geo_point_2d_b', 'geo_point_2d_a')\n",
    "img = ds.tf.shade(agg, cmap=list(data['couleurs']), how='eq_hist')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie Algorithmie, Optimisation des trajets\n",
    "\n",
    "Nous avons réalisé une analyse exploratoire de nos données, à présent, nous allons tâcher de répondre à la problèmatique du meilleur trajet, qui est un problème similaire au problème du voyageur de commerce pour lequel nous souhaitons utiliser l'algorithme de [Christofides](https://fr.wikipedia.org/wiki/Algorithme_de_Christofides). Pour cela nous utiliserons un GPU en local, pour calculer les distances entre tous les lieux, et nous lancerons l'algorithme de recherche du plus court chemin entre tous les lieux soit $6921$ éléments distincts.\n",
    "\n",
    "Nous devons suivre le schéma suivant pour construire l'algorithme de Christofides :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Algorithme de Christofides](christofides.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul du meilleur trajet\n",
    "\n",
    "### Calcul des distances entre chaque lieu\n",
    "\n",
    "Nous relevons 200137 points d'intérêts sur la carte. Si nous descendons dans la structure hiérarchisé de nos données, nous comprenons qu'il s'agit geo_point_2d -> id_emplacement -> lieu + complement d'adresse -> lieu -> arrondissement. C'est à dire qu'un lieu regroupe des id_emplacement, i.e. des arbres.\n",
    "\n",
    "Comme il s'agit de trouver une tournée véhiculée, entre $6921$ lieux uniques, contenant eux-mêmes un certain nombre d'arbres (chacun), et que ces lieux sont reliés entre eux par une distance pouvant se calculer d'après la latitude et la longitude, nous utiliserons une représentation en graphe (arrête et sommet) pour trouver le meilleurs chemins, en commençant par le calcul de la matrice des distances, puis du chemin de poids minimum.\n",
    "\n",
    "1. Nous calculerons la quantité d'arbres d'un lieu par la fonction d'aggrégeage \"aggfunc\" ci-dessous. Nous disposerons le résultat dans une colonne portant le nom de aire, car c'est une donnée quantitative que nous souhaitons représenter sous la forme de cercle avec plus ou moins de surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['arrondissement', 'lieu', 'geo_point_2d_a', 'geo_point_2d_b']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieu_aire = pd.DataFrame(df.pivot_table(index=['lieu'], aggfunc='size'), columns=['aire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lieu_aire.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nous éliminons ainsi les lieux doublons**, notons que les bulles de circonférences sont en rapport avec le nombre de ligne (et donc d'arbre) disparaissant dans cette opération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='lieu', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_1 = set(list(lieu_aire.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_2 = set(list(df['lieu']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_2 == set_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7 = \"\"\"SELECT  df.lieu as lieu,\n",
    "                df.arrondissement as arrond,\n",
    "                df.geo_point_2d_a as lat,\n",
    "                df.geo_point_2d_b as lon,\n",
    "                x.aire as aire\n",
    "                FROM df INNER JOIN x ON df.lieu == x.lieu ORDER BY lieu\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = ps.sqldf(q7, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de couleurs de bulles en fonction de chacun des arrondissements\n",
    "\n",
    "Les bulles de nos lieux seront munies d'une couleur dans notre graphe pour distinguer les arrondissements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couleurs_list = ['#B22222',\n",
    "         '#F08080',\n",
    "         '#DC143C',\n",
    "         '#FF0000',\n",
    "         '#FF4500',\n",
    "         '#FF8C00',\n",
    "         '#FFE4B5',\n",
    "         '#32CD32',\n",
    "         '#008000',\n",
    "         '#90EE90',\n",
    "         '#808000',\n",
    "         '#7FFFD4',\n",
    "         '#66CDAA',\n",
    "         '#48D1CC',\n",
    "         '#00CED1',\n",
    "         '#008080',\n",
    "         '#87CEFA',\n",
    "         '#1E90FF',\n",
    "         '#6495ED',\n",
    "         '#0000FF',\n",
    "         '#000080',\n",
    "         '#7B68EE',\n",
    "         '#EE82EE',\n",
    "         '#8A2BE2',\n",
    "         '#FF69B4'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couleurs_dict = dict()\n",
    "for it, val in enumerate(df_graph['arrond'].unique()):\n",
    "    couleurs_dict.update({val: couleurs_list[it]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph['couleurs'] = df_graph['arrond'].map(couleurs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['couleurs'] = data['arrond'].map(couleurs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous exportons notre tableau trier par ordre alphabétique de lieu pour l'utiliser sur notre gpu et pour que nous réalisions le calcul de la matrice des distances entre les lieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.to_excel('export_df_graph.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Nous calculerons la matrices des distances entre chaque lieu sur un gpu (le code est fourni en annexe), et ce sera donc une matrice de (6921 x 6921). Une distance est donnée par la relation $\\sqrt{((x_1 - x_0)*111)^2 + ((y_1 - y_0)*80)^2} ,\\forall x \\in latitude, \\forall  y\\in longitude$ en France, pour la distance en $km$. Le script de calcul de cette matrice est réalisé dans l'IDE PyCharm avec la bibliothèque PyCuda et une carte NVIDIA, localement. L'écriture du code en C ne se prête hélas pas tellement, visuellement parlant, au Notebook de Jupyter.\n",
    "\n",
    "En sortie nous traitons notre matrice pour la retrouver dans une dataframe avec nos indexes et colonnes en nom de lieu. Comme les calculs ont été réalisés dans des \"numpy arrays\" les colonnes n'ont pas été nommées. Mais nous avons effectuer les calculs sur un échantillon des données avec notre cpu, puis nous importerons le fichier de résultats de 600Mo dans une DataFrame pour les comparer et voir que nous trouvons le même résultat sur un échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_cpu = pd.DataFrame(df_graph, index=df_graph['lieu'].unique(), columns=df_graph['lieu'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for it, row in enumerate(distances_cpu.iloc[:5,:5].columns):\n",
    "    for it, col in enumerate(distances_cpu.iloc[:5,:5].columns):\n",
    "        distances_cpu[col].loc[row] = math.sqrt(((df_graph['lat'].loc[df_graph['lieu'] == col].values - df_graph['lat'].loc[df_graph['lieu'] == row].values)*111)**2\\\n",
    "+ ((df_graph['lon'].loc[df_graph['lieu'] == col].values - df_graph['lon'].loc[df_graph['lieu'] == row].values)*80)**2)\n",
    "\n",
    "distances_cpu.iloc[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## Code GPU de calcul de la matrice des distances \n",
    "\n",
    "Compte-tenu qu'il est nécessaire d'utiliser Anaconda, cela contreviendrait à la consigne qui stipule que l'exercice doit intégrer un environnement virtuel propre. Mais le code est présenté pour preuve :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-42a5507c4cd9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     17\u001B[0m \"\"\"\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m \u001B[0mmatrice\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_excel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'export_df_graph.xlsx'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m \u001B[0mNOMBRE\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmatrice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0mMATRIX_SIZE\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mNOMBRE\u001B[0m\u001B[0;31m#len(na.iloc[:NOMBRE])\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "kernel_code_template = \"\"\"\n",
    "__global__ void MatrixMulKernel(double* a, double* b, double* c)\n",
    "{\n",
    " \n",
    "    for (int k = 0; k <= %(MATRIX_SIZE)s; ++k) {\n",
    "            for (int l = 0; l <= %(MATRIX_SIZE)s; ++l) {\n",
    "                    double Aelement = a[l] - a[k];\n",
    "                    double Belement = b[l] - b[k];\n",
    "                    c[%(MATRIX_SIZE)s * k + l] = sqrt((Aelement*111)*(Aelement*111) + (Belement*80)*(Belement*80));\n",
    "                    }\n",
    "    }\n",
    "\n",
    "\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "matrice = pd.read_excel('export_df_graph.xlsx')\n",
    "NOMBRE = len(matrice)\n",
    "MATRIX_SIZE = NOMBRE#len(na.iloc[:NOMBRE])\n",
    "\n",
    "a_cpu = np.array(matrice['lat'].iloc[:NOMBRE]).reshape((NOMBRE, 1)).astype(np.float64())\n",
    "b_cpu = np.array(matrice['lon'].iloc[:NOMBRE]).reshape((NOMBRE, 1)).astype(np.float64())\n",
    "\n",
    "c_cpu = np.dot(a_cpu, b_cpu.T)\n",
    "a_gpu = gpuarray.to_gpu(a_cpu)\n",
    "b_gpu = gpuarray.to_gpu(b_cpu)\n",
    "\n",
    "c_gpu = gpuarray.empty((6921, 6921), np.float64())\n",
    "\n",
    "kernel_code = kernel_code_template % {\n",
    "    'MATRIX_SIZE': MATRIX_SIZE\n",
    "}\n",
    "\n",
    "mod = compiler.SourceModule(kernel_code)\n",
    "\n",
    "matrixmul = mod.get_function(\"MatrixMulKernel\")\n",
    "\n",
    "matrixmul(\n",
    "    a_gpu, b_gpu,\n",
    "    c_gpu,\n",
    "    block=(5, 5, 1),\n",
    ")\n",
    "\n",
    "result = c_gpu.get()\n",
    "\n",
    "\n",
    "col_new = dict()\n",
    "for x in range(6920):\n",
    "    col_new.update({x: matrice['lieu'].unique()[x]})\n",
    "\n",
    "df_result = pd.DataFrame(result)\n",
    "print(df_result)\n",
    "df_result = df_result.rename(columns=col_new, index=col_new)\n",
    "#df_result.to_excel('distances_gpu.xlsx')\n",
    "df_result = df_result.iloc[:1001,:1001]\n",
    "df_result.to_excel('distances_gpu.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous importons les résultats des calculs fait à partir de notre gpu local, sur l'heuristique des distances entre lieux. Ce fichier fait 600 Méga Octets.\n",
    "Nous voyons que les échantillons ci-dessus, et ci-dessous ont des résultats semblables. Comme le chargement du fichier prend trop de temps, nous avons pris le soin de produire un échantillon de 25 lieux, et c'est lui que nous chargeons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distances_gpu = pd.read_excel('distances_gpu.xlsx')\n",
    "#distances_gpu = pd.read_excel('short_distances.xlsx').drop(['Unnamed: 0'], axis=1).rename(columns={'Unnamed: 0.1':'lieu'})\n",
    "distances_gpu = pd.read_excel('distances_gpu.xlsx').drop(['Unnamed: 0'], axis=1).rename(columns={'Unnamed: 0.1':'lieu'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous comparons le résultat avec le tableau ci-dessous pour comprendre que tout s'est bien passé sur notre gpu et que nous pouvons travailler avec nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_gpu['lieu'] = distances_gpu.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_gpu.index = distances_gpu['lieu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_gpu.drop(['lieu'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_gpu.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sauvegardons un échantillon de notre matrice pour pouvoir effectuer le programme sur 25 lieux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distances.iloc[:25,:26].to_excel('short_distances.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problème du voyageur de commerce, algorithme de Christofides\n",
    "\n",
    "Nous considérons un graphe G(V,E) dont les poids respectent l'égalité triangulaire $d_{ij}+d_{jk}\\le d_{ik}$.\n",
    "\n",
    "La première étape de l'écriture de cet algorithme est l'écriture un arbre couvrant le poids minimum.\n",
    "Nous vous le présentons ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 50 #  Nombre de lieu à parcourir pour revenir à son point de départ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = distances_gpu.iloc[:SIZE,:SIZE].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici comment nous retournerons les valeurs des distances entre nos points, ce sont des kilomètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.loc[['44 ENFANTS D\\'IZIEU'],['28 BOULEVARD DE DOUAUMONT']].values[0][0]# Selectionner à partir de l'index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = distances_gpu.copy()#.iloc[:SIZE,:SIZE+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for i in itertools.permutations([1,2,3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = random.choice(d.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visite = {}#  villes visitées\n",
    "#set_visite = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.loc[d[start] == d[start].sort_values(ascending=True, ignore_index=True, kind='mergesort').iloc[2]].index[0]# mergesort O(nlog(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visite.update({num_int: start})# nous ajoutons une ville de départ\n",
    "#set_visite.add(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(visite, set_visite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d[[start]].sort_values(ascending=True,by=[start], axis=0,  kind='mergesort')#.iloc[0].index[1]# mergesort O(nlog(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ord_ville = d[[start]].sort_values(ascending=True,by=[start], axis=0,  kind='mergesort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ord_ville.iloc[0:].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set({ord_ville.iloc[0:].index[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous prenons la ligne de la ville la plus proche\n",
    "visite = dict()\n",
    "set_visite = set()\n",
    "start = random.choice(d.columns)\n",
    "visite.update({0:start})\n",
    "set_visite.add(start)\n",
    "next_ville = start\n",
    "next_try = ''\n",
    "km = float()\n",
    "for pos_chem in range(SIZE):\n",
    "    next_try = d[[next_ville]].sort_values(ascending=True, by=[next_ville], axis=0,  kind='mergesort')\n",
    "    for proxima in range(SIZE):#  Pour la taille\n",
    "        next_ville = next_try.iloc[proxima:].index[0]\n",
    "        if SIZE == len(visite):\n",
    "            print('Chemin de poids minimum terminé')\n",
    "            break\n",
    "        elif set_visite.isdisjoint(set({next_ville})):# not in visite.values()\n",
    "            visite.update({pos_chem+1: next_ville})\n",
    "            set_visite.add(next_ville)\n",
    "            km += d.loc[[visite[pos_chem]],[visite[pos_chem+1]]].values[0][0]\n",
    "            break\n",
    "    if pos_chem == SIZE -1:\n",
    "        km += d.loc[[visite[pos_chem]],[visite[0]]].values[0][0]\n",
    "print(visite, 'soit : ', int(km), 'km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat du chemin de poids minimal à partir d'un départ aléatoir est présenté ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite nous Calculons l'ensemble des sommets impairs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impaire_ensemble = dict()\n",
    "for v in range(len(visite)):\n",
    "    if v%2==1:\n",
    "        impaire_ensemble.update({v: visite[v]}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impaire_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous calculons un couplage de poids minimum dans l'ensemble des sommets impairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impaire_d = d[list(impaire_ensemble.values())].loc[list(impaire_ensemble.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impaire_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impaire_visite = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impaire_d[['ALLEE ROYALE']].sort_values(ascending=True, by=[next_ville], axis=0,  kind='mergesort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_list = [lieu for v,lieu in impaire_ensemble.items()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impaire_visite.update({num_int: impaire_ensemble[start]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impaire_dist = impaire_d[impaire_ensemble[start]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(list(range(len(impaire_d))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impaire_d[[next_ville]].sort_values(ascending=True, by=[next_ville], axis=0,  kind='mergesort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous prenons la ligne de la ville la plus proche\n",
    "imp_visite = dict()\n",
    "imp_set_visite = set()\n",
    "start = impaire_d.index[0]\n",
    "imp_visite.update({1:start})\n",
    "imp_set_visite.add(start)\n",
    "next_ville = start\n",
    "next_try = ''\n",
    "imp_km = float()\n",
    "for pos_chem in range(1, SIZE, 2):\n",
    "    #print(pos_chem)\n",
    "    next_try = impaire_d[[next_ville]].sort_values(ascending=True, by=[next_ville], axis=0,  kind='mergesort')\n",
    "    for proxima in range(1,len(impaire_d)):#  Pour la taille\n",
    "        #print(proxima, len(next_try), next_try.index)\n",
    "        #print(next_try, proxima)\n",
    "        next_ville = next_try.iloc[proxima:].index[0]\n",
    "        if len(imp_visite) == len(impaire_d):\n",
    "            print('Chemin impair de poids minimum terminé')\n",
    "            break\n",
    "        elif imp_set_visite.isdisjoint(set({next_ville})):# not in visite.values()\n",
    "            imp_visite.update({pos_chem+2: next_ville})\n",
    "            imp_set_visite.add(next_ville)\n",
    "          #  print(pos_chem, imp_visite[pos_chem])\n",
    "            imp_km += impaire_d.loc[[imp_visite[pos_chem]],[imp_visite[pos_chem+2]]].values[0][0]\n",
    "            break\n",
    "print(imp_visite, 'soit : ', int(imp_km), 'km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons faire l'union du couplage ci-dessus et de l'arbre couvrant de poids minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final = {}\n",
    "union = deque()\n",
    "for k,v in visite.items():\n",
    "    if k%2==0:\n",
    "        union.append({k: v})\n",
    "    if k%2==1:\n",
    "        if v == imp_visite[k]:\n",
    "            union.append({k: v})\n",
    "        if v != imp_visite[k]:\n",
    "            double_edge = set({v, imp_visite[k]})\n",
    "            union.append({k: double_edge})\n",
    "union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = {}\n",
    "union = deque()\n",
    "for k,v in visite.items():\n",
    "    if k%2==0:\n",
    "        union.append(v)\n",
    "    if k%2==1:\n",
    "        if v == imp_visite[k]:\n",
    "            union.append( v)\n",
    "        if v != imp_visite[k]:\n",
    "            double_edge = list()\n",
    "            double_edge.append(v)\n",
    "            double_edge.append(imp_visite[k])\n",
    "            union.append(double_edge)\n",
    "union = pd.DataFrame(list(union), columns=['lieu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = union.iloc[3]#.loc[union['lieu'] == '{ANDRE BRECHET (21) MAT, ALBIN HALLER 5}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler_tour = [ a[0][0], a[0][1], union.iloc[4][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p for p in itertools.permutations(list(euler_tour))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final = {}\n",
    "union = deque()\n",
    "for k,v in visite.items():\n",
    "    if k%2==0:\n",
    "        union.append({k: v})\n",
    "    if k%2==1:\n",
    "        if v == imp_visite[k]:\n",
    "            union.append({k: v})\n",
    "        if v != imp_visite[k]:\n",
    "            double_edge = set({v, imp_visite[k]})\n",
    "            union.append({k: double_edge})\n",
    "#n = 0 # to continue one loop to increment the deque\n",
    "km_final = float(0)\n",
    "km = float(0)\n",
    "km_second = float(0)\n",
    "i = 0\n",
    "for n in range(SIZE):\n",
    "    print('i ',i)\n",
    "    chemin = union[i]\n",
    "    # Pour les deux premiers, pas d'impair, ce sont des strings\n",
    "    if i<=1:\n",
    "        final.update({i: chemin[i]})\n",
    "        i+=1\n",
    "    elif isinstance(chemin[i], str):\n",
    "        voie_0 = list(chemin.values())[0]\n",
    "        #final.update({i: chemin[i]})\n",
    "        if i>1:\n",
    "            if voie_0 in list(final.values()):\n",
    "                print('merde')\n",
    "                i+=1\n",
    "                continue\n",
    "            km_final += d.loc[[final[i-1]],[voie_0]].values[0][0]\n",
    "            final.update({i-1: final[i-1], i: voie_0})\n",
    "            print(i,final[i-1], voie_0)\n",
    "            print(i,final[i-1],final[i])\n",
    "        # Comme le premier est distant de zéro avec lui-même\n",
    "        # nous prenons ici le cas de la mesure de la première distance\n",
    "        elif i==1:\n",
    "            km_final += d.loc[[final[i-1]],[final[i]]].values[0][0]\n",
    "        i+=1\n",
    "    # Si nous avons un impair, nous affectons les 4 valeurs en entrées\n",
    "    elif isinstance(chemin[i], set):\n",
    "        #print(final[i-1])\n",
    "        print('i : ',i, chemin[i])\n",
    "        voie_0 = final[i-1] # Cette valeur est le dernier lieu visité\n",
    "        voie_1 = chemin[i].pop()\n",
    "        voie_2 = chemin[i].pop()\n",
    "        if voie_1 in list(final.values()):\n",
    "            if voie_2 in list(final.values()):\n",
    "                i+=1\n",
    "                continue\n",
    "            else:\n",
    "                final.update({i: voie_2})\n",
    "                km_final += d.loc[[final[i-1]],[final[i]]].values[0][0]\n",
    "                print('none : ', n)\n",
    "                i+=1\n",
    "                continue\n",
    "        elif voie_2 in list(final.values()):\n",
    "            if voie_1 in list(final.values()):\n",
    "                print('none : ', n)\n",
    "                i+=1\n",
    "                continue\n",
    "            else:\n",
    "                final.update({i: voie_1})\n",
    "                km_final += d.loc[[final[i-1]],[final[i]]].values[0][0]\n",
    "                i+=1\n",
    "                continue\n",
    "            #final.update({i: voie_2})\n",
    "            #km_final += d.loc[[final[i-1]],[final[i]]].values[0][0]\n",
    "            #continue\n",
    "            \n",
    "        # Lorsque le tour arrive à la fin, il revient au début\n",
    "        if i < SIZE - 1:\n",
    "            voie_3 = list(union[i+1].values())[0]\n",
    "        if i == SIZE - 1:\n",
    "            voie_3 = final[0] # le dernier lieu et le premier sont les mêmes\n",
    "        \n",
    "        sort_tri = set()\n",
    "        euler_tour = [voie_1, voie_2, voie_3]\n",
    "        euler_tour = {p for p in itertools.permutations(euler_tour)}\n",
    "        sort_tri.add(voie_1)\n",
    "        sort_tri.add(voie_2)\n",
    "        sort_tri.add(voie_3)\n",
    "        km = d.loc[[voie_0],[voie_1]].values[0][0] + d.loc[[voie_1],[voie_2]].values[0][0] + d.loc[[voie_2],[voie_3]].values[0][0]\n",
    "        \n",
    "        for k in range(len(sort_tri)):\n",
    "            final.update({i+k: sort_tri.pop()})\n",
    "            \n",
    "        for j in range(5):\n",
    "            tour_ = euler_tour.pop()\n",
    "            voie_1_second = tour_[0]\n",
    "            voie_2_second = tour_[1]\n",
    "            voie_3_second = tour_[2]\n",
    "            km_second = d.loc[[voie_0],[voie_1_second]].values[0][0]+ d.loc[[voie_1_second],[voie_2_second]].values[0][0]\\\n",
    "                + d.loc[[voie_2_second],[voie_3_second]].values[0][0]\n",
    "            \n",
    "            if km > km_second:\n",
    "                sort_tri = set()\n",
    "                sort_tri.add(voie_1_second)\n",
    "                sort_tri.add(voie_2_second)\n",
    "                sort_tri.add(voie_3_second)\n",
    "        for k in range(len(sort_tri)):\n",
    "            final.update({i+k: sort_tri.pop()})\n",
    "        final.update({i-1: voie_0})\n",
    "        print(i, voie_0, voie_1, voie_2, voie_3)\n",
    "        print(i,final[i-1],final[i],final[i+1],final[i+2])\n",
    "        km_final += d.loc[[final[i-1]],[final[i]]].values[0][0]+ d.loc[[final[i]],[final[i+1]]].values[0][0]\\\n",
    "                + d.loc[[final[i+1]],[final[i+2]]].values[0][0]\n",
    "        i+=1\n",
    "print(final, km_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(final.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[x.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chemin[1].pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chemin = 'ALLEE DES JUSTES DE FRANCE'\n",
    "voie_1 = '48 BOULEVARD DE DOUAUMONT'\n",
    "voie_2 = 'ALLEE VIVALDI'\n",
    "voie_3 = 'ALLEE DES LAPINS'\n",
    "euler_tour = [voie_1, voie_2, voie_3]\n",
    "euler_tour = {p for p in itertools.permutations(euler_tour)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len (euler_tour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tour_ = euler_tour.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tour_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.loc[[voie_1],[voie_2]].values[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort_tri = set()\n",
    "km = float\n",
    "chemin = 'ALLEE DES JUSTES DE FRANCE'\n",
    "voie_1 = tour_.pop()\n",
    "voie_2 = tour_.pop()\n",
    "voie_3 = tour_.pop()\n",
    "sort_tri.add(voie_1)\n",
    "sort_tri.add(voie_2)\n",
    "sort_tri.add(voie_3)\n",
    "km = d.loc[[chemin],[voie_1]].values[0][0]+ d.loc[[voie_1],[voie_2]].values[0][0] + d.loc[[voie_2],[voie_3]].values[0][0]\n",
    "for i in range(5):\n",
    "    voie_1_second = tour_.pop()\n",
    "    voie_2_second = tour_.pop()\n",
    "    voie_3_second = tour_.pop()\n",
    "    km_second = d.loc[[chemin],[voie_1_second]].values[0][0]+ d.loc[[voie_1_second],[voie_2_second]].values[0][0]\\\n",
    "                + d.loc[[voie_2_second],[voie_3_second]].values[0][0]\n",
    "    print(sort_tri, km, km_second)\n",
    "    if km > km_second:\n",
    "        sort_tri = set()\n",
    "        sort_tri.add(voie_1_second)\n",
    "        sort_tri.add(voie_2_second)\n",
    "        sort_tri.add(voie_3_second)\n",
    "print(sort_tri, km, km_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons le tour Eulérien et le tour le plus court en même temps :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(lieu_un, lieu_deux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chemin = OrderedDict()\n",
    "for i in range(4):\n",
    "    enter = union.popitem(last=False)\n",
    "    if isinstance(enter[1], str):\n",
    "        lieu_1 = enter\n",
    "    elif isinstance(enter[1], set):\n",
    "        lieu_2 = enter[1].pop()\n",
    "        lieu_3 = enter[1].pop()\n",
    "        out = union.popitem(last=False)\n",
    "        \n",
    "        print([p for p in itertools.permutations([lieu_1,lieu_2,lieu_3,lieu_4])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIZE = 25 #  Nombre de lieu à parcourir pour revenir à son point de départ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = distances.iloc[:SIZE,:SIZE+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = union[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_int = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "christofides_solution = {}#  ville visitée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "christofides_solution.update({num_int: start})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "christ_dist = d[start].sort_values(ascending=True, ignore_index=True, kind='mergesort')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isinstance(union[3], set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "euler_tour = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "euler_tour.add(union[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "euler_tour.add(union[3].pop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{p for p in itertools.permutations(euler_tour)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d[union[1]].loc[d['lieu'] == union[3].pop()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "for i in range(SIZE-1):\n",
    "    n = 0\n",
    "    while True:#  si lieu déjà visité\n",
    "        if SIZE == len(christofides_solution):\n",
    "            print('Voyage terminé')\n",
    "            break\n",
    "        elif isinstance(union[n], set):\n",
    "            euler_tour = set()\n",
    "            euler_tour.add(union[n+2])\n",
    "            for i in range(2):\n",
    "                euler_tour.add(union[n+1].pop())\n",
    "            {p for p in itertools.permutations(euler_tour)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "for i in range(SIZE-1):\n",
    "    n = 0\n",
    "    while True:#  si lieu déjà visité\n",
    "        if SIZE == len(christofides_solution):\n",
    "            print('Voyage terminé')\n",
    "            break\n",
    "        # Si c'est la première fois que l'on visite ce lieu    \n",
    "        elif d['lieu'].loc[d[christofides_solution[num_int]] == christ_dist[n]].ravel()[0] not in christofides_solution.values():\n",
    "            # Rajouter le nom du lieu le plus proche dans la liste des solutions\n",
    "            christofides_solution.update({num_int + 1 : d['lieu'].loc[d[christofides_solution[num_int]] == christ_dist[n]].ravel()[0]}) \n",
    "            break\n",
    "        else :\n",
    "            n = n + 1\n",
    "    num_int = num_int + 1\n",
    "    christ_dist = d[christofides_solution[num_int]].sort_values(ascending=True, ignore_index=True, kind='mergesort')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, nous avons présenté les données de la ville de Paris, nous avons effectué une analyse univariée sur les éléments chiffrés, à la suite de quoi, nous avons calculé la matrice des distances entre les lieux, en ayant éliminé les doublons. Pour la présentation future du graphe, nous avons rajouté le nombre d'arbres par lieu dans la colonne aire, et une couleur distincte par arrondissement. Enfin nous avons résolu algorithmiquement le chemin de poids minimum pour n'importe quel point de départ. Par contre l'algorithme de Christofides étant de complexité $O(x) = x^3$, il me semble bien ambitieux pour 6921 lieux, et donc aussi pour le calculer dans ce projet dans des délais raisonnables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## Optimisation des ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = ds.Canvas(plot_width=850, plot_height=500)\n",
    "agg = cvs.points(df_graph, 'lon', 'lat')\n",
    "img = ds.tf.shade(agg, cmap=list(df_graph['couleurs']), how='eq_hist')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}